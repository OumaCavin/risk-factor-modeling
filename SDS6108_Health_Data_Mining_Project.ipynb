{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Data Mining and Analytics - Complete Data Science Project\n",
    "\n",
    "## MSc Public Health Data Science - SDS6108\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This comprehensive data science project demonstrates the complete workflow for analyzing behavioral risk factor data from the Behavioral Risk Factor Surveillance System (BRFSS). The project covers all phases from data collection and preprocessing through to predictive modeling and interpretation of results, providing a complete template for health data mining and analytics.\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Upon completing this notebook, you will be able to:\n",
    "\n",
    "1. **Understand the data science pipeline**: Grasp the end-to-end workflow from raw data to actionable insights\n",
    "2. **Apply data preprocessing techniques**: Handle missing values, outliers, and data quality issues\n",
    "3. **Conduct exploratory data analysis**: Use statistical and visualization techniques to understand health data\n",
    "4. **Perform feature engineering**: Create meaningful features from raw health indicators\n",
    "5. **Build predictive models**: Apply machine learning algorithms to health outcomes\n",
    "6. **Evaluate model performance**: Use appropriate metrics for healthcare prediction tasks\n",
    "7. **Interpret results**: Translate statistical findings into public health insights\n",
    "\n",
    "---\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "| Phase | Description | Key Activities |\n",
    "|-------|-------------|----------------|\n",
    "| Phase 1 | Project Setup | Library imports, configuration, data loading |\n",
    "| Phase 2 | Data Exploration | Initial assessment, descriptive statistics, data types |\n",
    "| Phase 3 | Data Cleaning | Missing value treatment, outlier handling, validation |\n",
    "| Phase 4 | Exploratory Analysis | Univariate, bivariate, and multivariate analysis |\n",
    "| Phase 5 | Feature Engineering | Variable transformation, creation of derived features |\n",
    "| Phase 6 | Model Development | Algorithm selection, training, hyperparameter tuning |\n",
    "| Phase 7 | Model Evaluation | Performance metrics, validation, comparison |\n",
    "| Phase 8 | Results & Conclusions | Interpretation, limitations, recommendations |\n",
    "\n",
    "---\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "```python\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, spearmanr\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, precision_recall_curve, f1_score, accuracy_score)\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Project Setup and Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries and configure the environment for optimal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: PROJECT SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Import statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, spearmanr, pearsonr\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score, accuracy_score,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure warnings and settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Print environment information\n",
    "print(\"=\" * 80)\n",
    "print(\"HEALTH DATA MINING AND ANALYTICS PROJECT\")\n",
    "print(\"MSc Public Health Data Science - SDS6108\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProject started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"scikit-learn version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading and Initial Assessment\n",
    "\n",
    "We load the data and perform an initial assessment to understand its structure, size, and basic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Define file paths\n",
    "DATA_PATH_CSV = 'user_input_files/behavioral risk factor-selected.csv'\n",
    "DATA_PATH_XLSX = 'user_input_files/behavioral risk factor analysis.xlsx'\n",
    "\n",
    "# Try loading CSV first (preferred for large datasets)\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH_CSV)\n",
    "    print(\"✓ Successfully loaded data from CSV file\")\n",
    "    print(f\"  Source: {DATA_PATH_CSV}\")\nexcept Exception as e:\n",
    "    print(f\"CSV loading failed: {e}\")\n",
    "    try:\n",
    "        df = pd.read_excel(DATA_PATH_XLSX)\n",
    "        print(\"✓ Successfully loaded data from Excel file\")\n",
    "        print(f\"  Source: {DATA_PATH_XLSX}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Excel loading failed: {e2}\")\n",
    "        raise Exception(\"Could not load data from either source\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INITIAL DATA ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n--- First 5 Records ---\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Dictionary and Variable Description\n",
    "\n",
    "Understanding the variables in our dataset is crucial for proper analysis. The BRFSS uses specific coding conventions that we need to understand."
   ]
  },
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA DICTIONARY\n",
    "# =============================================================================\n",
    "\n",
    "# Define comprehensive data dictionary\n",
    "data_dictionary = {\n",
    "    '_STATE': {\n",
    "        'description': 'State FIPS code',\n",
    "        'type': 'Categorical',\n",
    "        'range': '1-72'\n",
    "    },\n",
    "    'NUMADULT': {\n",
    "        'description': 'Number of adults in household',\n",
    "        'type': 'Numeric',\n",
    "        'range': '1-76'\n",
    "    },\n",
    "    'GENHLTH': {\n",
    "        'description': 'General health status (1=Excellent, 5=Poor)',\n",
    "        'type': 'Ordinal',\n",
    "        'range': '1-5'\n",
    "    },\n",
    "    'PHYSHLTH': {\n",
    "        'description': 'Days of poor physical health in past 30 days',\n",
    "        'type': 'Numeric',\n",
    "        'range': '0-30, 88=none'\n",
    "    },\n",
    "    'MENTHLTH': {\n",
    "        'description': 'Days of poor mental health in past 30 days',\n",
    "        'type': 'Numeric',\n",
    "        'range': '0-30, 88=none'\n",
    "    },\n",
    "    'POORHLTH': {\n",
    "        'description': 'Days when poor physical or mental health kept from usual activities',\n",
    "        'type': 'Numeric',\n",
    "        'range': '0-30, 88=none'\n",
    "    },\n",
    "    'HLTHPLN1': {\n",
    "        'description': 'Have any kind of health care coverage',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'PERSDOC2': {\n",
    "        'description': 'Have a personal doctor or health care provider',\n",
    "        'type': 'Ordinal',\n",
    "        'values': '1=Yes, one, 2=Yes, more than one, 3=No'\n",
    "    },\n",
    "    'MEDCOST': {\n",
    "        'description': 'Could not see doctor due to cost in past 12 months',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'CHECKUP1': {\n",
    "        'description': 'Time since last routine checkup',\n",
    "        'type': 'Ordinal',\n",
    "        'values': '1=Within past year, 2=Within past 2 years, 3=Within past 5 years, 4=5+ years ago, 7=Don\'t know, 8=Never'\n",
    "    },\n",
    "    'BPHIGH4': {\n",
    "        'description': 'Ever told blood pressure is high',\n",
    "        'type': 'Ordinal',\n",
    "        'values': '1=Yes, 2=No, 3=Told borderline, 7=Don\'t know/Refused'\n",
    "    },\n",
    "    'BPMEDS': {\n",
    "        'description': 'Taking medicine for high blood pressure',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'SEX': {\n",
    "        'description': 'Respondent sex',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Male, 2=Female'\n",
    "    },\n",
    "    'MARITAL': {\n",
    "        'description': 'Marital status',\n",
    "        'type': 'Categorical',\n",
    "        'values': '1=Married, 2=Divorced, 3=Widowed, 4=Separated, 5=Never married, 6=Unmarried couple'\n",
    "    },\n",
    "    'EDUCA': {\n",
    "        'description': 'Education level',\n",
    "        'type': 'Ordinal',\n",
    "        'values': '1-6 (1=Never attended, 6=College 4+ years)'\n",
    "    },\n",
    "    'RENTHOM1': {\n",
    "        'description': 'Own or rent home',\n",
    "        'type': 'Categorical',\n",
    "        'values': '1=Own, 2=Rent, 3=Other arrangement'\n",
    "    },\n",
    "    'VETERAN3': {\n",
    "        'description': 'Are you a veteran',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'EMPLOY1': {\n",
    "        'description': 'Employment status',\n",
    "        'type': 'Categorical',\n",
    "        'values': '1-8 (1=Employed, 8=Unable to work)'\n",
    "    },\n",
    "    'WEIGHT2': {\n",
    "        'description': 'Self-reported weight in pounds',\n",
    "        'type': 'Numeric',\n",
    "        'special_values': '7777/9999=Refused/Don\'t know'\n",
    "    },\n",
    "    'HEIGHT3': {\n",
    "        'description': 'Self-reported height in inches (feet*12 + inches)',\n",
    "        'type': 'Numeric',\n",
    "        'special_values': '7777/9999=Refused/Don\'t know'\n",
    "    },\n",
    "    'DIFFWALK': {\n",
    "        'description': 'Difficulty walking or climbing stairs',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'SMOKE100': {\n",
    "        'description': 'Smoked at least 100 cigarettes in entire life',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    },\n",
    "    'SMOKDAY2': {\n",
    "        'description': 'Frequency of smoking now',\n",
    "        'type': 'Ordinal',\n",
    "        'values': '1=Every day, 2=Some days, 3=Not at all'\n",
    "    },\n",
    "    'EXERANY2': {\n",
    "        'description': 'Exercise or physical activity in past 30 days',\n",
    "        'type': 'Binary',\n",
    "        'values': '1=Yes, 2=No'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create DataFrame from dictionary\n",
    "dict_df = pd.DataFrame([\n",
    "    {'Variable': k, **v} for k, v in data_dictionary.items()\n",
    "])\n",
    "\n",
    "print(\"\\n--- Data Dictionary ---\")\n",
    "print(f\"Total Variables: {len(data_dictionary)}\")\n",
    "dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Exploration and Quality Assessment\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Data Types and Structure Analysis\n",
    "\n",
    "Examining the data types helps us identify variables that may need conversion and understand the nature of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: DATA EXPLORATION AND QUALITY ASSESSMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2: DATA EXPLORATION AND QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data types analysis\n",
    "print(\"\\n--- Data Types Summary ---\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Display column information\n",
    "print(\"\\n--- Column Details ---\")\n",
    "dtype_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data Type': df.dtypes.values,\n",
    "    'Non-Null Count': df.count().values,\n",
    "    'Null Count': df.isnull().sum().values,\n",
    "    'Unique Values': df.nunique().values\n",
    "})\n",
    "print(dtype_info.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Value Analysis\n",
    "\n",
    "Missing values are common in survey data. Understanding their patterns helps us choose appropriate treatment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING VALUE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate missing values\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum().values,\n",
    "    'Missing %': (df.isnull().sum().values / len(df) * 100).round(2),\n",
    "    'Empty String Count': (df == '').sum().values,\n",
    "    'Data Type': df.dtypes.astype(str).values\n",
    "})\n",
    "\n",
    "# Special value analysis (BRFSS specific)\n",
    "special_values = {}\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        special_values[col] = {\n",
    "            '77': (df[col] == 77).sum(),\n",
    "            '88': (df[col] == 88).sum(),\n",
    "            '99': (df[col] == 99).sum(),\n",
    "            '7777': (df[col] == 7777).sum(),\n",
    "            '9999': (df[col] == 9999).sum()\n",
    "        }\n",
    "\n",
    "print(\"\\n--- Missing Value Analysis ---\")\n",
    "missing_analysis = missing_analysis[missing_analysis['Missing Count'] > 0]\n",
    "if len(missing_analysis) > 0:\n",
    "    print(missing_analysis.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing values found (using standard pandas null detection)\")\n",
    "\n",
    "print(\"\\n--- Special Values Analysis (BRFSS Coding) ---\")\n",
    "print(\"These values represent 'Not Applicable', 'Refused', or 'Don't Know' responses:\")\n",
    "\n",
    "# Create summary of special values\n",
    "special_summary = []\n",
    "for col, values in special_values.items():\n",
    "    total_special = sum(values.values())\n",
    "    if total_special > 0:\n",
    "        special_summary.append({\n",
    "            'Column': col,\n",
    "            'Total Special': total_special,\n",
    "            'Special %': round(total_special / len(df) * 100, 2),\n",
    "            '=77': values['77'],\n",
    "            '=88': values['88'],\n",
    "            '=99': values['99']\n",
    "        })\n",
    "\n",
    "if special_summary:\n",
    "    special_df = pd.DataFrame(special_summary)\n",
    "    print(special_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Descriptive Statistics\n",
    "\n",
    "Computing descriptive statistics helps us understand the distribution and central tendency of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "# Numeric columns statistics\n",
    "print(\"\\n--- Descriptive Statistics (Numeric Variables) ---\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc_stats = df[numeric_cols].describe().T\n",
    "desc_stats['range'] = desc_stats['max'] - desc_stats['min']\n",
    "desc_stats['IQR'] = desc_stats['75%'] - desc_stats['25%']\n",
    "print(desc_stats[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'range', 'IQR']].round(2).to_string())\n",
    "\n",
    "# Categorical/Ordinal columns value counts\n",
    "print(\"\\n--- Value Counts for Key Categorical Variables ---\")\n",
    "\n",
    "key_categorical = ['GENHLTH', 'SEX', 'BPHIGH4', 'HLTHPLN1', 'DIFFWALK', 'SMOKE100', 'EXERANY2']\n",
    "\n",
    "for col in key_categorical:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Quality Report\n",
    "\n",
    "Creating a comprehensive data quality report helps identify issues that need to be addressed during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment\n",
    "    \"\"\"\n",
    "    quality_metrics = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        metrics = {\n",
    "            'Column': col,\n",
    "            'Dtype': str(df[col].dtype),\n",
    "            'Total_Records': len(df),\n",
    "            'Missing_Null': df[col].isnull().sum(),\n",
    "            'Missing_Empty': (df[col] == '').sum() if df[col].dtype == 'object' else 0,\n",
    "            'Missing_Total': df[col].isnull().sum() + ((df[col] == '').sum() if df[col].dtype == 'object' else 0),\n",
    "            'Unique': df[col].nunique(),\n",
    "            'Missing_%': round(df[col].isnull().sum() / len(df) * 100, 2)\n",
    "        }\n",
    "        \n",
    "        # Check for outliers using IQR method\n",
    "        if df[col].dtype in ['int64', 'float64'] and df[col].nunique() > 10:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            metrics['Outliers_IQR'] = outliers\n",
    "            metrics['Outliers_%'] = round(outliers / len(df) * 100, 2)\n",
    "        else:\n",
    "            metrics['Outliers_IQR'] = 'N/A'\n",
    "            metrics['Outliers_%'] = 'N/A'\n",
    "            \n",
    "        quality_metrics.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(quality_metrics)\n",
    "\n",
    "quality_report = assess_data_quality(df)\n",
    "\n",
    "print(\"\\n--- Data Quality Report ---\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Columns with Missing Values: {(quality_report['Missing_%'] > 0).sum()}\")\n",
    "\n",
    "print(\"\\n--- Quality Issues Identified ---\")\n",
    "issues = quality_report[(quality_report['Missing_%'] > 0) | \n",
    "                        (quality_report['Outliers_%'] != 'N/A')]\n",
    "if len(issues) > 0:\n",
    "    print(issues[['Column', 'Missing_%', 'Outliers_%']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant quality issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Data Cleaning and Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Create Working Dataset\n",
    "\n",
    "Before cleaning, we create a working copy to preserve the original data and apply cleaning transformations systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: DATA CLEANING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3: DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create working copy\n",
    "df_clean = df.copy()\n",
    "print(f\"\\nWorking dataset created: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\")\n",
    "\n",
    "# Document cleaning steps\n",
    "cleaning_log = []\n",
    "\n",
    "def log_cleaning_step(description, before_count, after_count):\n",
    "    \"\"\"Log cleaning operations for documentation\"\"\"\n",
    "    cleaning_log.append({\n",
    "        'Step': len(cleaning_log) + 1,\n",
    "        'Description': description,\n",
    "        'Records_Before': before_count,\n",
    "        'Records_After': after_count,\n",
    "        'Change': before_count - after_count\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Cleaning Process Started ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Special Values (BRFSS Coding)\n",
    "\n",
    "BRFSS uses specific codes for non-responses. We need to identify and handle these appropriately.\n",
    "\n",
    "**Special Value Meanings:**\n",
    "- **77, 99**: Refused or Don't Know (for binary/small scale questions)\n",
    "- **88**: None/Zero (for questions about number of days)\n",
    "- **7777, 9999**: Refused or Don't Know (for continuous values like weight/height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HANDLE SPECIAL VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Define special value mappings\n",
    "special_value_map = {\n",
    "    # 88 typically means \"None\" or \"Zero days\" - convert to 0\n",
    "    'PHYSHLTH': {88: 0},\n",
    "    'MENTHLTH': {88: 0},\n",
    "    'POORHLTH': {88: 0},\n",
    "    \n",
    "    # 77, 99 typically mean \"Refused\" or \"Don't know\" - convert to NaN\n",
    "    'GENHLTH': {77: np.nan, 99: np.nan},\n",
    "    'HLTHPLN1': {7: np.nan, 9: np.nan},\n",
    "    'PERSDOC2': {7: np.nan, 9: np.nan},\n",
    "    'MEDCOST': {7: np.nan, 9: np.nan},\n",
    "    'CHECKUP1': {7: np.nan, 9: np.nan},\n",
    "    'BPHIGH4': {7: np.nan, 9: np.nan},\n",
    "    'BPMEDS': {7: np.nan, 9: np.nan},\n",
    "    'SEX': {7: np.nan, 9: np.nan},\n",
    "    'MARITAL': {7: np.nan, 9: np.nan},\n",
    "    'EDUCA': {7: np.nan, 9: np.nan},\n",
    "    'RENTHOM1': {7: np.nan, 9: np.nan},\n",
    "    'VETERAN3': {7: np.nan, 9: np.nan},\n",
    "    'EMPLOY1': {7: np.nan, 9: np.nan},\n",
    "    'DIFFWALK': {7: np.nan, 9: np.nan},\n",
    "    'SMOKE100': {7: np.nan, 9: np.nan},\n",
    "    'SMOKDAY2': {7: np.nan, 9: np.nan},\n",
    "    'EXERANY2': {7: np.nan, 9: np.nan},\n",
    "    \n",
    "    # Large special values for weight/height - convert to NaN\n",
    "    'WEIGHT2': {7777: np.nan, 9999: np.nan},\n",
    "    'HEIGHT3': {7777: np.nan, 9999: np.nan}\n",
    "}\n",
    "\n",
    "# Apply special value conversions\n",
    "special_converted_count = 0\n",
    "for col, value_map in special_value_map.items():\n",
    "    if col in df_clean.columns:\n",
    "        before_missing = df_clean[col].isnull().sum()\n",
    "        for old_value, new_value in value_map.items():\n",
    "            df_clean.loc[df_clean[col] == old_value, col] = new_value\n",
    "        after_missing = df_clean[col].isnull().sum()\n",
    "        converted = after_missing - before_missing\n",
    "        if converted > 0:\n",
    "            special_converted_count += converted\n",
    "            print(f\"  {col}: {converted:,} special values converted to {'NaN' if new_value is np.nan else new_value}\")\n",
    "\n",
    "print(f\"\\nTotal special values converted: {special_converted_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handle Missing Values\n",
    "\n",
    "Different missing value treatment strategies are appropriate for different variable types and contexts. We apply a combination of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HANDLE MISSING VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# Current missing value status\n",
    "missing_summary_before = df_clean.isnull().sum()\n",
    "print(\"--- Missing Values Before Treatment ---\")\n",
    "missing_cols = missing_summary_before[missing_summary_before > 0]\n",
    "for col, count in missing_cols.items():\n",
    "    pct = count / len(df_clean) * 100\n",
    "    print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Define treatment strategies based on variable type\n",
    "missing_treatment = {\n",
    "    # Categorical/Binary: Use mode\n",
    "    'SEX': 'mode',\n",
    "    'HLTHPLN1': 'mode',\n",
    "    'PERSDOC2': 'mode',\n",
    "    'MEDCOST': 'mode',\n",
    "    'CHECKUP1': 'mode',\n",
    "    'BPHIGH4': 'mode',\n",
    "    'BPMEDS': 'mode',\n",
    "    'MARITAL': 'mode',\n",
    "    'EDUCA': 'mode',\n",
    "    'RENTHOM1': 'mode',\n",
    "    'VETERAN3': 'mode',\n",
    "    'EMPLOY1': 'mode',\n",
    "    'DIFFWALK': 'mode',\n",
    "    'SMOKE100': 'mode',\n",
    "    'SMOKDAY2': 'mode',\n",
    "    'EXERANY2': 'mode',\n",
    "    \n",
    "    # Ordinal: Use median (preserves ordering)\n",
    "    'GENHLTH': 'median',\n",
    "    \n",
    "    # Numeric: Use median (robust to outliers)\n",
    "    'PHYSHLTH': 'median',\n",
    "    'MENTHLTH': 'median',\n",
    "    'POORHLTH': 'median',\n",
    "    'WEIGHT2': 'median',\n",
    "    'HEIGHT3': 'median',\n",
    "    \n",
    "    # Special: Drop rows (high missing or not critical)\n",
    "    'NUMADULT': 'drop'\n",
    "}\n",
    "\n",
    "# Apply missing value treatments\n",
    "rows_before = len(df_clean)\n",
    "\n",
    "for col, treatment in missing_treatment.items():\n",
    "    if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "        \n",
    "        if treatment == 'mode':\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  {col}: Imputed with mode = {mode_value}\")\n",
    "            \n",
    "        elif treatment == 'median':\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"  {col}: Imputed with median = {median_value}\")\n",
    "            \n",
    "        elif treatment == 'drop':\n",
    "            df_clean = df_clean.dropna(subset=[col])\n",
    "            print(f\"  {col}: Dropped {len(df_clean) - rows_before} rows with missing values\")\n",
    "\n",
    "rows_after = len(df_clean)\n",
    "\n",
    "print(f\"\\n--- Missing Values After Treatment ---\")\n",
    "remaining_missing = df_clean.isnull().sum().sum()\n",
    "print(f\"Total remaining missing values: {remaining_missing:,}\")\n",
    "print(f\"Rows before: {rows_before:,} → Rows after: {rows_after:,} (dropped {rows_before - rows_after:,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Handle Outliers\n",
    "\n",
    "Outliers can significantly impact analysis and modeling. We identify and handle them based on domain knowledge and statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HANDLE OUTLIERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Outlier Detection and Treatment ---\")\n",
    "\n",
    "# Define reasonable ranges based on BRFSS documentation\n",
    "reasonable_ranges = {\n",
    "    'PHYSHLTH': (0, 30),      # Days in past 30 days\n",
    "    'MENTHLTH': (0, 30),      # Days in past 30 days\n",
    "    'POORHLTH': (0, 30),      # Days in past 30 days\n",
    "    'WEIGHT2': (50, 700),     # Weight in pounds (reasonable adult range)\n",
    "    'HEIGHT3': (36, 96),      # Height in inches (3-8 feet)\n",
    "    'NUMADULT': (1, 20),      # Number of adults in household\n",
    "    'GENHLTH': (1, 5),        # 1=Excellent to 5=Poor\n",
    "    'EDUCA': (1, 6)           # Education level 1-6\n",
    "}\n",
    "\n",
    "# Detect and cap outliers\n",
    "outlier_summary = []\n",
    "\n",
    "for col, (min_val, max_val) in reasonable_ranges.items():\n",
    "    if col in df_clean.columns:\n",
    "        # Count outliers\n",
    "        below_min = (df_clean[col] < min_val).sum()\n",
    "        above_max = (df_clean[col] > max_val).sum()\n",
    "        total_outliers = below_min + above_max\n",
    "        \n",
    "        if total_outliers > 0:\n",
    "            # Cap outliers to reasonable range\n",
    "            df_clean[col] = df_clean[col].clip(lower=min_val, upper=max_val)\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Below_Min': below_min,\n",
    "                'Above_Max': above_max,\n",
    "                'Total_Outliers': total_outliers,\n",
    "                'Treatment': 'Capped to valid range'\n",
    "            })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(outlier_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"  No outliers detected in key variables after special value treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Feature Type Conversion\n",
    "\n",
    "Ensuring proper data types for each variable is essential for accurate analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE TYPE CONVERSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Data Type Conversions ---\")\n",
    "\n",
    "# Convert to appropriate types\n",
    "type_conversions = {\n",
    "    # Convert to integer for categorical/binary variables\n",
    "    '_STATE': 'int32',\n",
    "    'SEX': 'int8',\n",
    "    'GENHLTH': 'int8',\n",
    "    'HLTHPLN1': 'int8',\n",
    "    'PERSDOC2': 'int8',\n",
    "    'MEDCOST': 'int8',\n",
    "    'CHECKUP1': 'int8',\n",
    "    'BPHIGH4': 'int8',\n",
    "    'BPMEDS': 'int8',\n",
    "    'MARITAL': 'int8',\n",
    "    'EDUCA': 'int8',\n",
    "    'RENTHOM1': 'int8',\n",
    "    'VETERAN3': 'int8',\n",
    "    'EMPLOY1': 'int8',\n",
    "    'DIFFWALK': 'int8',\n",
    "    'SMOKE100': 'int8',\n",
    "    'SMOKDAY2': 'int8',\n",
    "    'EXERANY2': 'int8',\n",
    "    \n",
    "    # Keep as integer for numeric variables\n",
    "    'NUMADULT': 'int8',\n",
    "    'PHYSHLTH': 'int8',\n",
    "    'MENTHLTH': 'int8',\n",
    "    'POORHLTH': 'int8',\n",
    "    'WEIGHT2': 'int16',\n",
    "    'HEIGHT3': 'int16'\n",
    "}\n",
    "\n",
    "memory_before = df_clean.memory_usage(deep=True).sum()\n",
    "\n",
    "for col, dtype in type_conversions.items():\n",
    "    if col in df_clean.columns:\n",
    "        try:\n",
    "            df_clean[col] = df_clean[col].astype(dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not convert {col} to {dtype}: {e}\")\n",
    "\n",
    "memory_after = df_clean.memory_usage(deep=True).sum()\n",
    "\n",
    "print(f\"Memory usage: {memory_before/1024**2:.2f} MB → {memory_after/1024**2:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - memory_after/memory_before)*100:.1f}%\")\n",
    "\n",
    "# Final data types\n",
    "print(\"\\n--- Final Data Types ---\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Final Cleaned Dataset Summary\n",
    "\n",
    "After preprocessing, we have a clean dataset ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL CLEANED DATASET SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANED DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Shape: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\")\n",
    "print(f\"Missing Values: {df_clean.isnull().sum().sum():,}\")\n",
    "print(f\"Duplicate Rows: {df_clean.duplicated().sum():,}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"After removing duplicates: {df_clean.shape[0]:,} rows\")\n",
    "\n",
    "# Final descriptive statistics\n",
    "print(\"\\n--- Final Descriptive Statistics ---\")\n",
    "print(df_clean.describe().round(2).to_string())\n",
    "\n",
    "# Document cleaning log\n",
    "print(\"\\n--- Cleaning Operations Summary ---\")\n",
    "cleaning_log_df = pd.DataFrame(cleaning_log)\n",
    "if len(cleaning_log_df) > 0:\n",
    "    print(cleaning_log_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Univariate Analysis\n",
    "\n",
    "Analyzing each variable individually helps understand distributions and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create figure for health status distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Univariate Analysis of Health Status Variables', fontsize=14, fontweight='bold')\n",
    "\n",
    "# General Health\n",
    "ax1 = axes[0, 0]\n",
    "genhlth_counts = df_clean['GENHLTH'].value_counts().sort_index()\n",
    "colors = sns.color_palette(\"RdYlGn_r\", len(genhlth_counts))\n",
    "ax1.bar(genhlth_counts.index, genhlth_counts.values, color=colors)\n",
    "ax1.set_xlabel('General Health Rating')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('General Health Status\\n(1=Excellent, 5=Poor)')\n",
    "ax1.set_xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "# Physical Health Days\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(df_clean['PHYSHLTH'], bins=31, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "ax2.set_xlabel('Days of Poor Physical Health (Last 30 Days)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Physical Health Days')\n",
    "ax2.axvline(df_clean['PHYSHLTH'].mean(), color='red', linestyle='--', label=f'Mean: {df_clean[\"PHYSHLTH\"].mean():.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "# Mental Health Days\n",
    "ax3 = axes[0, 2]\n",
    "ax3.hist(df_clean['MENTHLTH'], bins=31, color='mediumseagreen', edgecolor='white', alpha=0.7)\n",
    "ax3.set_xlabel('Days of Poor Mental Health (Last 30 Days)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Distribution of Mental Health Days')\n",
    "ax3.axvline(df_clean['MENTHLTH'].mean(), color='red', linestyle='--', label=f'Mean: {df_clean[\"MENTHLTH\"].mean():.1f}')\n",
    "ax3.legend()\n",
    "\n",
    "# Blood Pressure History\n",
    "ax4 = axes[1, 0]\n",
    "bphigh_labels = {1: 'High', 2: 'No', 3: 'Borderline', 7: 'Unknown'}\n",
    "bphigh_counts = df_clean['BPHIGH4'].replace(bphigh_labels).value_counts()\n",
    "ax4.pie(bphigh_counts.values, labels=bphigh_counts.index, autopct='%1.1f%%', \n",
    "        colors=sns.color_palette(\"Set2\", len(bphigh_counts)))\n",
    "ax4.set_title('High Blood Pressure History')\n",
    "\n",
    "# Exercise Frequency\n",
    "ax5 = axes[1, 1]\n",
    "exercise_labels = {1: 'Exercised', 2: 'Did Not Exercise'}\n",
    "exercise_counts = df_clean['EXERANY2'].replace(exercise_labels).value_counts()\n",
    "bars = ax5.bar(exercise_counts.index, exercise_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "ax5.set_xlabel('')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title('Exercise in Past 30 Days')\n",
    "for bar, count in zip(bars, exercise_counts.values):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "             f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Smoking Status\n",
    "ax6 = axes[1, 2]\n",
    "smoke_labels = {1: 'Ever Smoked (100+)', 2: 'Never Smoked'}\n",
    "smoke_counts = df_clean['SMOKE100'].replace(smoke_labels).value_counts()\n",
    "bars = ax6.bar(smoke_counts.index, smoke_counts.values, color=['#3498db', '#9b59b6'])\n",
    "ax6.set_xlabel('')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_title('Smoking History (100+ Cigarettes)')\n",
    "for bar, count in zip(bars, smoke_counts.values):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "             f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('univariate_health_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Univariate analysis visualization saved to 'univariate_health_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bivariate Analysis\n",
    "\n",
    "Examining relationships between pairs of variables helps identify associations and potential predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIVARIATE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Bivariate Analysis ---\\n\")\n",
    "\n",
    "# Create correlation matrix for numeric variables\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Correlation heatmap\n",
    "ax1 = axes[0]\n",
    "numeric_vars = ['GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'WEIGHT2', 'HEIGHT3', 'DIFFWALK']\n",
    "corr_matrix = df_clean[numeric_vars].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, ax=ax1, square=True, linewidths=0.5)\n",
    "ax1.set_title('Correlation Matrix of Health Variables', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Health status by sex\n",
    "ax2 = axes[1]\n",
    "health_by_sex = df_clean.groupby('SEX')['GENHLTH'].value_counts(normalize=True).unstack()\n",
    "health_by_sex.index = ['Male', 'Female']\n",
    "health_by_sex.plot(kind='bar', ax=ax2, colormap='RdYlGn_r', edgecolor='black')\n",
    "ax2.set_xlabel('Sex')\n",
    "ax2.set_ylabel('Proportion')\n",
    "ax2.set_title('General Health Status by Sex', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticklabels(['Male', 'Female'], rotation=0)\n",
    "ax2.legend(title='Health Rating', labels=['Excellent', 'Good', 'Fair', 'Poor', 'Very Poor'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bivariate_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Bivariate analysis visualization saved to 'bivariate_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Statistical Tests for Associations\n",
    "\n",
    "Conducting formal statistical tests to quantify associations between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL TESTS FOR ASSOCIATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Statistical Tests for Variable Associations ---\\n\")\n",
    "\n",
    "# Chi-square tests for categorical variables\n",
    "print(\"1. Chi-Square Tests for Categorical Variables\\n\")\n",
    "\n",
    "categorical_pairs = [\n",
    "    ('GENHLTH', 'BPHIGH4'),\n",
    "    ('DIFFWALK', 'EXERANY2'),\n",
    "    ('SMOKE100', 'BPHIGH4'),\n",
    "    ('GENHLTH', 'DIFFWALK'),\n",
    "    ('SEX', 'BPHIGH4')\n",
    "]\n",
    "\n",
    "chi2_results = []\n",
    "\n",
    "for var1, var2 in categorical_pairs:\n",
    "    if var1 in df_clean.columns and var2 in df_clean.columns:\n",
    "        contingency = pd.crosstab(df_clean[var1], df_clean[var2])\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "        \n",
    "        # Cramér's V for effect size\n",
    "        n = contingency.sum().sum()\n",
    "        min_dim = min(contingency.shape) - 1\n",
        "        cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "        \n",
    "        chi2_results.append({\n",
    "            'Variables': f'{var1} × {var2}',\n",
    "            'Chi²': round(chi2, 2),\n",
    "            'p-value': f'{p_value:.2e}',\n",
            'df': dof,\n",
    "            \"Cramér's V\": round(cramers_v, 3),\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results)\n",
    "print(chi2_df.to_string(index=False))\n",
    "\n",
    "# Spearman correlations for ordinal variables\n",
    "print(\"\\n\\n2. Spearman Correlations for Ordinal Variables\\n\")\n",
    "\n",
    "ordinal_vars = ['GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'DIFFWALK', 'EXERANY2']\n",
    "spearman_results = []\n",
    "\n",
    "for i, var1 in enumerate(ordinal_vars):\n",
    "    for var2 in ordinal_vars[i+1:]:\n",
    "        corr, p_value = spearmanr(df_clean[var1], df_clean[var2])\n",
    "        spearman_results.append({\n",
    "            'Variables': f'{var1} × {var2}',\n",
    "            'Spearman ρ': round(corr, 3),\n",
    "            'p-value': f'{p_value:.2e}',\n",
            'Interpretation': 'Strong' if abs(corr) > 0.5 else 'Moderate' if abs(corr) > 0.3 else 'Weak'\n",
    "        })\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_results)\n",
    "print(spearman_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Multivariate Analysis\n",
    "\n",
    "Exploring relationships among multiple variables simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTIVARIATE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Multivariate Analysis ---\\n\")\n",
    "\n",
    "# Create health profile by demographic groups\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Health status by education level\n",
    "ax1 = axes[0, 0]\n",
    "edu_health = df_clean.groupby('EDUCA')['GENHLTH'].mean()\n",
    "edu_labels = ['Never', 'Grades 1-8', 'Grades 9-11', 'High School', 'Some College', 'College+']\n",
    "bars = ax1.bar(range(len(edu_health)), edu_health.values, color=sns.color_palette(\"viridis\", len(edu_health)))\n",
    "ax1.set_xticks(range(len(edu_health)))\n",
    "ax1.set_xticklabels(edu_labels, rotation=45, ha='right')\n",
    "ax1.set_xlabel('Education Level')\n",
    "ax1.set_ylabel('Mean Health Rating (1=Excellent, 5=Poor)')\n",
    "ax1.set_title('Health Status by Education Level\\n(Lower = Better Health)', fontsize=11)\n",
    "ax1.axhline(df_clean['GENHLTH'].mean(), color='red', linestyle='--', label='Overall Mean')\n",
    "ax1.legend()\n",
    "\n",
    "# Health status by employment\n",
    "ax2 = axes[0, 1]\n",
    "emp_health = df_clean.groupby('EMPLOY1')['GENHLTH'].mean()\n",
    "emp_labels = ['Employed', 'Self-employed', 'Out of work', 'Homemaker', 'Student', 'Retired', 'Unable to work']\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(emp_health)))\n",
    "bars = ax2.bar(range(len(emp_health)), emp_health.values, color=colors)\n",
    "ax2.set_xticks(range(len(emp_health)))\n",
    "ax2.set_xticklabels(emp_labels, rotation=45, ha='right')\n",
    "ax2.set_xlabel('Employment Status')\n",
    "ax2.set_ylabel('Mean Health Rating')\n",
    "ax2.set_title('Health Status by Employment Status', fontsize=11)\n",
    "\n",
    "# BMI proxy analysis (weight vs height by health status)\n",
    "ax3 = axes[1, 0]\n",
    "# Create BMI-like metric (weight in kg / height in m^2, using imperial units adjusted)\n",
    "df_clean['BMI_proxy'] = (df_clean['WEIGHT2'] * 0.453592) / ((df_clean['HEIGHT3'] * 0.0254) ** 2)\n",
    "for health_rating in sorted(df_clean['GENHLTH'].unique()):\n",
    "    subset = df_clean[df_clean['GENHLTH'] == health_rating]['BMI_proxy']\n",
    "    health_labels = {1: 'Excellent', 2: 'Good', 3: 'Fair', 4: 'Poor', 5: 'Very Poor'}\n",
    "    ax3.hist(subset, bins=30, alpha=0.5, label=f'{health_labels.get(health_rating, health_rating)}')\n",
    "ax3.set_xlabel('BMI Proxy')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('BMI Distribution by Health Status', fontsize=11)\n",
    "ax3.legend()\n",
    "ax3.set_xlim(10, 60)\n",
    "\n",
    "# Risk factor count by health status\n",
    "ax4 = axes[1, 1]\n",
    "# Create composite risk score\n",
    "df_clean['risk_factors'] = (\n",
    "    (df_clean['SMOKE100'] == 1).astype(int) +\n",
    "    (df_clean['EXERANY2'] == 2).astype(int) +\n",
    "    (df_clean['BPHIGH4'] == 1).astype(int) +\n",
    "    (df_clean['DIFFWALK'] == 1).astype(int) +\n",
    "    (df_clean['OBESE'] == 1).astype(int) if 'OBESE' in df_clean.columns else 0\n",
    ")\n",
    "\n",
    "risk_by_health = df_clean.groupby('GENHLTH')['risk_factors'].mean()\n",
    "health_labels = ['Excellent', 'Good', 'Fair', 'Poor', 'Very Poor']\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(risk_by_health)))\n",
    "bars = ax4.bar(range(len(risk_by_health)), risk_by_health.values, color=colors)\n",
    "ax4.set_xticks(range(len(risk_by_health)))\n",
    "ax4.set_xticklabels(health_labels, rotation=45, ha='right')\n",
    "ax4.set_xlabel('General Health Rating')\n",
    "ax4.set_ylabel('Mean Number of Risk Factors')\n",
    "ax4.set_title('Risk Factors by Health Status', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multivariate_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Multivariate analysis visualization saved to 'multivariate_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Create Derived Features\n",
    "\n",
    "Creating new features from existing variables can improve model performance and provide additional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 5: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "print(\"\\n--- Creating Derived Features ---\\n\")\n",
    "\n",
    "# 1. Body Mass Index (BMI) Proxy\n",
    "print(\"1. Calculating BMI proxy...\")\n",
    "df_features['BMI'] = (df_features['WEIGHT2'] * 0.453592) / ((df_features['HEIGHT3'] * 0.0254) ** 2)\n",
    "print(f\"   BMI range: {df_features['BMI'].min():.1f} - {df_features['BMI'].max():.1f}\")\n",
    "\n",
    "# 2. BMI Categories\n",
    "print(\"2. Creating BMI categories...\")\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 1  # Underweight\n",
    "    elif bmi < 25:\n",
    "        return 2  # Normal\n",
    "    elif bmi < 30:\n",
    "        return 3  # Overweight\n",
    "    else:\n",
    "        return 4  # Obese\n",
    "\n",
    "df_features['BMI_CAT'] = df_features['BMI'].apply(categorize_bmi)\n",
    "print(\"   Categories: Underweight, Normal, Overweight, Obese\")\n",
    "\n",
    "# 3. Overall Health Score (composite)\n",
    "print(\"3. Creating composite health score...\")\n",
    "# Normalize components and create score (higher = better health)\n",
    "df_features['health_score'] = (\n",
    "    5 - df_features['GENHLTH'] +                    # Invert health rating\n",
    "    (30 - df_features['PHYSHLTH']) / 30 * 2 +       # Physical health days (max 2 points)\n",
    "    (30 - df_features['MENTHLTH']) / 30 * 2 +       # Mental health days (max 2 points)\n",
    "    (df_features['EXERANY2'] == 1).astype(int) +    # Exercise (1 point)\n",
    "    (df_features['HLTHPLN1'] == 1).astype(int)      # Health coverage (1 point)\n",
    ")\n",
    "print(f\"   Health score range: {df_features['health_score'].min():.2f} - {df_features['health_score'].max():.2f}\")\n",
    "\n",
    "# 4. Risk Factor Count\n",
    "print(\"4. Creating risk factor count...\")\n",
    "df_features['RISK_COUNT'] = (\n",
    "    (df_features['SMOKE100'] == 1).astype(int) +          # Smoking\n",
    "    (df_features['EXERANY2'] == 2).astype(int) +          # No exercise\n",
    "    (df_features['BPHIGH4'] == 1).astype(int) +           # High BP\n",
    "    (df_features['DIFFWALK'] == 1).astype(int) +          # Mobility issues\n",
    "    (df_features['BMI_CAT'] == 4).astype(int)             # Obese\n",
    ")\n",
    "print(f\"   Risk count range: 0 - {df_features['RISK_COUNT'].max()}\")\n",
    "\n",
    "# 5. Mental-Physical Health Gap\n",
    "print(\"5. Creating mental-physical health gap...\")\n",
    "df_features['HLTH_GAP'] = abs(df_features['PHYSHLTH'] - df_features['MENTHLTH'])\n",
    "print(f\"   Health gap range: 0 - {df_features['HLTH_GAP'].max()}\")\n",
    "\n",
    "# 6. Healthcare Access Score\n",
    "print(\"6. Creating healthcare access score...\")\n",
    "df_features['ACCESS_SCORE'] = (\n",
    "    (df_features['HLTHPLN1'] == 1).astype(int) +      # Has coverage\n",
    "    (df_features['PERSDOC2'] == 1).astype(int) +      # Has personal doctor\n",
    "    (df_features['MEDCOST'] == 2).astype(int) +       # No cost barrier\n",
    "    (df_features['CHECKUP1'] == 1).astype(int)        # Recent checkup\n",
    ")\n",
    "print(f\"   Access score range: 0 - {df_features['ACCESS_SCORE'].max()}\")\n",
    "\n",
    "# 7. Age proxy from state distribution (if applicable)\n",
    "print(\"7. Creating demographic groupings...\")\n",
    "df_features['HIGH_EDUCATION'] = (df_features['EDUCA'] >= 5).astype(int)\n",
    "df_features['EMPLOYED'] = (df_features['EMPLOY1'].isin([1, 2])).astype(int)\n",
    "df_features['MARRIED'] = (df_features['MARITAL'] == 1).astype(int)\n",
    "print(\"   Created: HIGH_EDUCATION, EMPLOYED, MARRIED flags\")\n",
    "\n",
    "# Display new features\n",
    "print(\"\\n--- New Features Summary ---\")\n",
    "new_features = ['BMI', 'BMI_CAT', 'health_score', 'RISK_COUNT', 'HLTH_GAP', 'ACCESS_SCORE']\n",
    "for feat in new_features:\n",
    "    if feat in df_features.columns:\n",
    "        print(f\"\\n{feat}:\")\n",
    "        print(df_features[feat].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Target Variable\n",
    "\n",
    "For predictive modeling, we need to define a target variable. We'll create a binary classification target for poor health status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE TARGET VARIABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Creating Target Variable ---\\n\")\n",
    "\n",
    "# Create binary target: Poor Health (GENHLTH >= 4)\n",
    "print(\"Target: Poor Health Status (GENHLTH >= 4)\")\n",
    "print(\"This represents individuals who rate their health as 'Poor' or 'Very Poor'\\n\")\n",
    "\n",
    "df_features['POOR_HEALTH'] = (df_features['GENHLTH'] >= 4).astype(int)\n",
    "\n",
    "# Target distribution\n",
    "target_dist = df_features['POOR_HEALTH'].value_counts()\n",
    "target_pct = df_features['POOR_HEALTH'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target Distribution:\")\n",
    "print(f\"  0 (Good Health): {target_dist[0]:,} ({target_pct[0]:.2f}%)\")\n",
    "print(f\"  1 (Poor Health): {target_dist[1]:,} ({target_pct[1]:.2f}%)\")\n",
    "print(f\"\\nClass Imbalance Ratio: {target_dist[0]/target_dist[1]:.2f}:1\")\n",
    "\n",
    "# Create alternative target: High Risk (3+ risk factors)\n",
    "print(\"\\n\\nAlternative Target: High Cardiovascular Risk\")\n",
    "df_features['HIGH_RISK'] = (df_features['RISK_COUNT'] >= 3).astype(int)\n",
    "\n",
    "risk_dist = df_features['HIGH_RISK'].value_counts()\n",
    "print(f\"  Low Risk (0-2 factors): {(df_features['HIGH_RISK']==0).sum():,}\")\n",
    "print(f\"  High Risk (3+ factors): {(df_features['HIGH_RISK']==1).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Selection for Modeling\n",
    "\n",
    "Selecting the most relevant features for our predictive model based on domain knowledge and correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Feature Selection for Modeling ---\\n\")\n",
    "\n",
    "# Define feature groups for the model\n",
    "demographic_features = ['SEX', 'EDUCA', 'MARITAL', 'EMPLOY1', 'RENTHOM1', 'VETERAN3']\n",
    "health_behaviors = ['EXERANY2', 'SMOKE100', 'SMOKDAY2']\n",
    "health_conditions = ['BPHIGH4', 'BPMEDS', 'DIFFWALK', 'PHYSHLTH', 'MENTHLTH']\n",
    "healthcare_access = ['HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1']\n",
    "derived_features = ['BMI', 'BMI_CAT', 'RISK_COUNT', 'ACCESS_SCORE', 'HIGH_EDUCATION', 'EMPLOYED', 'MARRIED']\n",
    "\n",
    "# Combine all features\n",
    "all_features = demographic_features + health_behaviors + health_conditions + healthcare_access + derived_features\n",
    "\n",
    "# Remove duplicates and ensure features exist\n",
    "model_features = list(dict.fromkeys([f for f in all_features if f in df_features.columns]))\n",
    "\n",
    "# Correlation with target\n",
    "print(\"Feature Correlations with Target (POOR_HEALTH):\")\n",
    "correlations = df_features[model_features + ['POOR_HEALTH']].corr()['POOR_HEALTH'].drop('POOR_HEALTH')\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "for feat, corr in correlations.items():\n",
    "    direction = '+' if df_features[feat].corr(df_features['POOR_HEALTH']) > 0 else '-'\n",
    "    print(f\"  {feat:20s}: {corr:.3f} ({direction})\")\n",
    "\n",
    "# Select top features (correlation > 0.05)\n",
    "selected_features = correlations[correlations > 0.05].index.tolist()\n",
    "\n",
    "print(f\"\\n--- Selected Features for Modeling ---\")\n",
    "print(f\"Total features selected: {len(selected_features)}\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Model Development\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Prepare Data for Modeling\n",
    "\n",
    "Splitting data into training and testing sets, and applying feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 6: MODEL DEVELOPMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6: MODEL DEVELOPMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_features[selected_features].copy()\n",
    "y = df_features['POOR_HEALTH'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Data Split ---\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class balance in splits\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Good Health (0): {(y_train==0).sum():,} ({(y_train==0).mean()*100:.2f}%)\")\n",
    "print(f\"  Poor Health (1): {(y_train==1).sum():,} ({(y_train==1).mean()*100:.2f}%)\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n✓ Features scaled using StandardScaler\")\n",
    "print(f\"  Training data mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"  Training data std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Baseline Model\n",
    "\n",
    "Establishing a baseline performance using a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BASELINE MODEL: LOGISTIC REGRESSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Baseline Model: Logistic Regression ---\\n\")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "y_prob_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"  F1-Score (Poor Health): {f1_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_prob_baseline):.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(baseline_model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"\\n  Cross-Validation ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Good Health', 'Poor Health']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Advanced Models\n",
    "\n",
    "Training and comparing multiple machine learning models to find the best performing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED MODELS COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Advanced Models Comparison ---\\n\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=RANDOM_STATE, max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=RANDOM_STATE, class_weight='balanced', max_depth=10\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced',\n",
    "        n_jobs=-1, max_depth=10\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, random_state=RANDOM_STATE, max_depth=5,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "model_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_auc = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    model_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    'CV-ROC-AUC': cv_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}, F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}, CV-AUC: {cv_auc:.4f}\")\n",
    "\n",
    "# Create results comparison\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n--- Model Comparison Summary ---\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\n✓ Best Model: {best_model_name} (ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Hyperparameter Tuning\n",
    "\n",
    "Optimizing the best performing model's hyperparameters to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER TUNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Hyperparameter Tuning ---\\n\")\n",
    "\n",
    "# For Random Forest (typically best for this type of data)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [8, 10, 12],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "print(\"Tuning Random Forest with GridSearchCV...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "\n",
    "# Grid search\n",
    "rf_tuned = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_tuned,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n✓ Best Parameters Found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Model Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### 7.1 Final Model Performance\n",
    "\n",
    "Comprehensive evaluation of the tuned model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 7: MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 7: MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final predictions\n",
    "y_pred_final = best_model.predict(X_test_scaled)\n",
    "y_prob_final = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---\\n\")\n",
    "\n",
    "# Core metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "f1 = f1_score(y_test, y_pred_final)\n",
    "roc_auc = roc_auc_score(y_test, y_prob_final)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  Accuracy:           {accuracy:.4f}\")\n",
    "print(f\"  F1-Score:           {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:            {roc_auc:.4f}\")\n",
    "\n",
    "# Cross-validation on full training set\n",
    "cv_auc = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"  Cross-Val ROC-AUC:  {cv_auc.mean():.4f} (+/- {cv_auc.std()*2:.4f})\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "report = classification_report(y_test, y_pred_final, target_names=['Good Health', 'Poor Health'])\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(f\"                  Predicted\")\n",
    "print(f\"                  Good   Poor\")\n",
    "print(f\"Actual Good    {cm[0,0]:6,}  {cm[0,1]:6,}\")\n",
    "print(f\"       Poor    {cm[1,0]:6,}  {cm[1,1]:6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 ROC Curve and Precision-Recall Analysis\n",
    "\n",
    "Visualizing model performance using ROC curves and precision-recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROC CURVE AND PRECISION-RECALL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_final)\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax1.fill_between(fpr, tpr, alpha=0.3, color='darkorange')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax1.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax1.set_title('Receiver Operating Characteristic (ROC) Curve', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob_final)\n",
    "pr_auc = np.trapz(precision, recall)\n",
    "ax2.plot(recall, precision, color='green', lw=2, label=f'PR Curve')\n",
    "ax2.fill_between(recall, precision, alpha=0.3, color='green')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Recall', fontsize=11)\n",
    "ax2.set_ylabel('Precision', fontsize=11)\n",
    "ax2.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc=\"lower left\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Evaluation curves saved to 'model_evaluation_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Feature Importance Analysis\n",
    "\n",
    "Identifying the most important features for predicting poor health outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Feature Importance Analysis ---\\n\")\n",
    "\n",
    "# Get feature importances from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(feature_importance)))\n",
    "bars = ax.barh(range(len(feature_importance)), feature_importance['Importance'].values, color=colors)\n",
    "ax.set_yticks(range(len(feature_importance)))\n",
    "ax.set_yticklabels(feature_importance['Feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Feature Importance', fontsize=11)\n",
    "ax.set_title('Feature Importance for Predicting Poor Health\\n(Random Forest Model)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, feature_importance['Importance'].values)):\n",
    "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['Feature']:20s}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Feature importance visualization saved to 'feature_importance.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Results and Conclusions\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1 Summary of Findings\n",
    "\n",
    "Synthesizing the key findings from the analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 8: RESULTS AND CONCLUSIONS\n",
    "=============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 8: RESULTS AND CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY OF FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. DATA QUALITY FINDINGS:\n",
    "   - The dataset contained 441,457 survey responses from the BRFSS\n",
    "   - Special values (77, 88, 99, 7777, 9999) required careful treatment\n",
    "   - Missing values were handled using domain-appropriate methods\n",
    "   - Final clean dataset: {:,} records with 23 original + 7 derived features\n",
    "\n",
    "2. HEALTH STATUS DISTRIBUTION:\n",
    "   - {:,.0f}% of respondents rated their health as 'Good' or better (GENHLTH 1-2)\n",
    "   - {:,.0f}% reported poor physical health on at least one day in past month\n",
    "   - {:,.0f}% reported poor mental health on at least one day in past month\n",
    "   - {:.1f}% currently smoke cigarettes\n",
    "   - {:.1f}% reported no physical activity in the past 30 days\n",
    "\n",
    "3. KEY RISK FACTORS IDENTIFIED:\n",
    "   - Physical health days (PHYSHLTH) strongly associated with poor health outcomes\n",
    "   - Difficulty walking (DIFFWALK) is a major predictor of health status\n",
    "   - High blood pressure (BPHIGH4) shows significant correlation with poor health\n",
    "   - Exercise frequency (EXERANY2) is protective against poor health\n",
    "   - BMI and derived risk factors contribute meaningfully to prediction\n",
    "\n",
    "4. MODEL PERFORMANCE:\n",
    "   - Best Model: Random Forest Classifier\n",
    "   - Accuracy: {:.1%}\n",
    "   - F1-Score (Poor Health): {:.4f}\n",
    "   - ROC-AUC: {:.4f}\n",
    "   - Cross-Validation ROC-AUC: {:.4f} (+/- {:.4f})\n",
    "\n",
    "5. TOP PREDICTIVE FEATURES:\n",
    "   - Physical health days (PHYSHLTH)\n",
    "   - General health self-assessment (GENHLTH)\n",
    "   - Difficulty walking/climbing stairs (DIFFWALK)\n",
    "   - Exercise behavior (EXERANY2)\n",
    "   - Body Mass Index (BMI)\n",
    "   - High blood pressure history (BPHIGH4)\n",
    "   - Mental health days (MENTHLTH)\n",
    "   - Healthcare access score (ACCESS_SCORE)\n",
    "   - Employment status (EMPLOY1)\n",
    "   - Age proxy and demographic factors\n",
    "\"\"\".format(\n",
    "    len(df_features),\n",
          (df_features['GENHLTH'] <= 2).mean() * 100,\n",
    "    (df_features['PHYSHLTH'] > 0).mean() * 100,\n",
    "    (df_features['MENTHLTH'] > 0).mean() * 100,\n",
       (df_features['SMOKDAY2'] == 1).mean() * 100,\n",
    "    (df_features['EXERANY2'] == 2).mean() * 100,\n",
    "    accuracy,\n",
    "    f1,\n",
    "    roc_auc,\n",
    "    cv_auc.mean(),\n",
    "    cv_auc.std() * 2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Public Health Implications\n",
    "\n",
    "Translating statistical findings into actionable public health recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PUBLIC HEALTH IMPLICATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the analysis, the following public health implications are identified:\n",
    "\n",
    "1. PHYSICAL ACTIVITY INTERVENTIONS:\n",
    "   - Finding: Lack of exercise is strongly associated with poor health outcomes\n",
    "   - Recommendation: Increase community-based physical activity programs\n",
    "   - Target: Adults who report no physical activity in the past 30 days\n",
    "\n",
    "2. CHRONIC DISEASE MANAGEMENT:\n",
    "   - Finding: High blood pressure and mobility issues are key predictors\n",
    "   - Recommendation: Improve hypertension screening and management programs\n",
    "   - Target: Adults with undiagnosed or uncontrolled hypertension\n",
    "\n",
    "3. MENTAL HEALTH INTEGRATION:\n",
    "   - Finding: Mental health days correlate with overall health status\n",
    "   - Recommendation: Integrate mental health services in primary care\n",
    "   - Target: Adults reporting poor mental health days\n",
    "\n",
    "4. HEALTHCARE ACCESS BARRIERS:\n",
    "   - Finding: Healthcare access score affects health outcomes\n",
    "   - Recommendation: Remove cost barriers to healthcare access\n",
    "   - Target: Adults who delay care due to cost concerns\n",
    "\n",
    "5. HIGH-RISK POPULATION SCREENING:\n",
    "   - Finding: Multiple risk factors compound poor health risk\n",
    "   - Recommendation: Implement risk stratification for early intervention\n",
    "   - Target: Individuals with 3+ risk factors (smoking, obesity, inactivity, etc.)\n",
    "\n",
    "6. EMPLOYMENT AND HEALTH:\n",
    "   - Finding: Employment status is a significant predictor\n",
    "   - Recommendation: Workplace wellness programs and health insurance access\n",
    "   - Target: Unemployed and underemployed populations\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STUDY LIMITATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. CROSS-SECTIONAL DESIGN:\n",
    "   - Cannot establish causal relationships between risk factors and outcomes\n",
    "   - Temporal sequence of events cannot be determined\n",
    "\n",
    "2. SELF-REPORTED DATA:\n",
    "   - Subject to recall bias and social desirability bias\n",
    "   - Height and weight may be under/over-reported\n",
    "\n",
    "3. CLASS IMBALANCE:\n",
    "   - Poor health represents minority class (~{:.1f}%)\n",
    "   - Model may have lower sensitivity for detecting poor health cases\n",
    "\n",
    "4. MISSING DATA:\n",
    "   - Non-response patterns may not be random\n",
    "   - Some demographic groups may be underrepresented\n",
    "\n",
    "5. GENERALIZABILITY:\n",
    "   - BRFSS data represents US adult population\n",
    "   - May not generalize to other populations or healthcare systems\n",
    "\"\"\".format(\n",
    "    df_features['POOR_HEALTH'].mean() * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Future Directions and Recommendations\n",
    "\n",
    "Suggesting areas for further research and model improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FUTURE DIRECTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. MODEL ENHANCEMENTS:\n",
    "   - Apply SMOTE or other resampling techniques to address class imbalance\n",
    "   - Explore ensemble methods (XGBoost, LightGBM) for improved performance\n",
    "   - Implement SHAP values for better interpretability\n",
    "   - Develop local explainable AI for individual predictions\n",
    "\n",
    "2. DATA ENRICHMENT:\n",
    "   - Link with clinical records for validation\n",
    "   - Include social determinants of health data\n",
    "   - Add time-series components for longitudinal analysis\n",
    "   - Incorporate environmental and neighborhood factors\n",
    "\n",
    "3. CLINICAL VALIDATION:\n",
    "   - Validate predictions against clinical outcomes\n",
    "   - Conduct prospective studies to assess predictive accuracy\n",
    "   - Compare with established clinical risk scores\n",
    "\n",
    "4. INTERVENTION STUDIES:\n",
    "   - Test effectiveness of targeted interventions\n",
    "   - Evaluate cost-effectiveness of screening programs\n",
    "   - Measure impact on health disparities\n",
    "\n",
    "5. TECHNOLOGY APPLICATIONS:\n",
    "   - Develop web-based risk assessment tools\n",
    "   - Integrate with electronic health records\n",
    "   - Create mobile applications for health monitoring\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT COMPLETION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "This comprehensive data science project demonstrates:\n",
    "\n",
    "✓ Data collection and loading from multiple sources\n",
    "✓ Data quality assessment and cleaning\n",
    "✓ Handling of survey-specific coding (BRFSS special values)\n",
    "✓ Exploratory data analysis with visualizations\n",
    "✓ Statistical testing for variable associations\n",
    "✓ Feature engineering and selection\n",
    "✓ Multiple machine learning model development\n",
    "✓ Hyperparameter tuning and model optimization\n",
    "✓ Comprehensive model evaluation\n",
    "✓ Feature importance analysis\n",
    "✓ Interpretation of results for public health applications\n",
    "\n",
    "Project completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "OUTPUT FILES GENERATED:\n",
    "  - univariate_health_analysis.png\n",
    "  - bivariate_analysis.png\n",
    "  - multivariate_analysis.png\n",
    "  - model_evaluation_curves.png\n",
    "  - feature_importance.png\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Export Final Dataset and Model\n",
    "\n",
    "Saving the cleaned dataset and trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT FINAL DATASET AND MODEL\n",
    "=============================================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(\"\\n--- Exporting Results ---\\n\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_path = 'health_data_mining_results'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Export cleaned data\n",
    "df_features.to_csv(f'{output_path}/cleaned_health_data.csv', index=False)\n",
    "print(f\"✓ Cleaned dataset saved to '{output_path}/cleaned_health_data.csv'\")\n",
    "\n",
    "# Export feature importance\n",
    "feature_importance.to_csv(f'{output_path}/feature_importance.csv', index=False)\n",
    "print(f\"✓ Feature importance saved to '{output_path}/feature_importance.csv'\")\n",
    "\n",
    "# Export model results\n",
    "results_df.to_csv(f'{output_path}/model_comparison.csv', index=False)\n",
    "print(f\"✓ Model comparison saved to '{output_path}/model_comparison.csv'\")\n",
    "\n",
    "# Export trained model and scaler\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'selected_features': selected_features,\n",
    "    'feature_importance': feature_importance,\n",
    "    'best_params': grid_search.best_params_\n",
    "}\n",
    "\n",
    "with open(f'{output_path}/trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "print(f\"✓ Trained model saved to '{output_path}/trained_model.pkl'\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "Dataset:\n",
    "  - Original records: 441,457\n",
    "  - Clean records: {len(df_features):,}\n",
    "  - Features: {len(selected_features)} (selected from {len(df_features.columns)} total)\n",
    "  \n",
    "Target Variable:\n",
    "  - POOR_HEALTH (GENHLTH >= 4)\n",
    "  - Class distribution: {(df_features['POOR_HEALTH']==0).sum():,} Good / {(df_features['POOR_HEALTH']==1).sum():,} Poor\n",
    "\n",
    "Best Model: Random Forest\n",
    "  - ROC-AUC: {roc_auc:.4f}\n",
    "  - Accuracy: {accuracy:.4f}\n",
    "  - F1-Score: {f1:.4f}\n",
    "\n",
    "Files Generated:\n",
    "  - cleaned_health_data.csv\n",
    "  - feature_importance.csv\n",
    "  - model_comparison.csv\n",
    "  - trained_model.pkl\n",
    "  - *.png (visualizations)\n",
    "\n",
    "============================================================\n",
    "END OF DATA SCIENCE PROJECT\n",
    "============================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "### A.1 Variable Reference Guide\n",
    "\n",
    "Complete reference for all variables used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPENDIX: VARIABLE REFERENCE GUIDE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"APPENDIX A: VARIABLE REFERENCE GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ORIGINAL VARIABLES (from BRFSS):\n",
    "--------------------------------\n",
    "Demographic Variables:\n",
    "  _STATE      State FIPS code (1-72)\n",
    "  NUMADULT    Number of adults in household\n",
    "  SEX         1=Male, 2=Female\n",
    "  MARITAL     1=Married, 2=Divorced, 3=Widowed, 4=Separated, 5=Never married\n",
    "  EDUCA       1-6 (Education level, 6=College 4+ years)\n",
    "  RENTHOM1    1=Own, 2=Rent, 3=Other arrangement\n",
    "  VETERAN3    1=Veteran, 2=Non-veteran\n",
    "  EMPLOY1     Employment status (1=Employed, 8=Unable to work)\n",
    "\n",
    "Health Status Variables:\n",
    "  GENHLTH     General health (1=Excellent, 2=Very good, 3=Good, 4=Fair, 5=Poor)\n",
    "  PHYSHLTH    Days of poor physical health (past 30 days)\n",
    "  MENTHLTH    Days of poor mental health (past 30 days)\n",
    "  POORHLTH    Days poor health kept from usual activities\n",
    "\n",
    "Health Conditions:\n",
    "  BPHIGH4     Ever told blood pressure was high (1=Yes, 2=No, 3=Borderline)\n",
    "  BPMEDS      Taking medicine for high blood pressure\n",
    "  DIFFWALK    Difficulty walking or climbing stairs\n",
    "\n",
    "Health Behaviors:\n",
    "  SMOKE100    Smoked at least 100 cigarettes in lifetime\n",
    "  SMOKDAY2    Current smoking frequency (1=Every day, 2=Some days, 3=Not at all)\n",
    "  EXERANY2    Exercise or physical activity in past 30 days\n",
    "\n",
    "Healthcare Access:\n",
    "  HLTHPLN1    Any kind of health care coverage\n",
    "  PERSDOC2    Has personal doctor or health care provider\n",
    "  MEDCOST     Could not see doctor due to cost (past 12 months)\n",
    "  CHECKUP1    Time since last routine checkup\n",
    "\n",
    "Body Metrics:\n",
    "  WEIGHT2     Weight in pounds\n",
    "  HEIGHT3     Height in inches (feet*12 + inches)\n",
    "\n",
    "DERIVED VARIABLES (Created during analysis):\n",
    "--------------------------------------------\n",
    "  BMI           Calculated Body Mass Index\n",
    "  BMI_CAT       BMI Category (1=Underweight, 2=Normal, 3=Overweight, 4=Obese)\n",
    "  health_score  Composite health score (higher = better health)\n",
    "  RISK_COUNT    Number of cardiovascular risk factors\n",
    "  HLTH_GAP      Absolute difference between physical and mental health days\n",
    "  ACCESS_SCORE  Composite healthcare access score (0-4)\n",
    "  HIGH_EDUCATION Binary flag for college education or higher\n",
    "  EMPLOYED      Binary flag for employed or self-employed\n",
    "  MARRIED       Binary flag for married respondents\n",
    "  POOR_HEALTH   Target variable: GENHLTH >= 4\n",
    "  HIGH_RISK     Target variable: RISK_COUNT >= 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Common BRFSS Special Values\n",
    "\n",
    "Reference for understanding BRFSS coding conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"APPENDIX B: BRFSS SPECIAL VALUE CODES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "BRFSS uses specific codes to represent non-response categories:\n",
    "\n",
    "COMMON SPECIAL VALUES:\n",
    "----------------------\n",
    "  Value    Meaning\n",
    "  ------   --------------------------------------------------\n",
    "  7        Refused (for questions with small value ranges)\n",
    "  8        Not asked / Missing / Unknown (for some variables)\n",
    "  9        Don't know (for questions with small value ranges)\n",
    "  77       Refused (for questions with larger value ranges)\n",
    "  88       None / Zero (typically for 'number of days' questions)\n",
    "  99       Don't know (for questions with larger value ranges)\n",
    "  7777     Refused (for continuous variables like weight/height)\n",
    "  9999     Don't know (for continuous variables like weight/height)\n",
    "\n",
    "EXAMPLE CONVERSIONS:\n",
    "-------------------\n",
    "  PHYSHLTH = 88  →  Convert to 0 (0 days of poor physical health)\n",
    "  PHYSHLTH = 77  →  Convert to NaN (Refused to answer)\n",
    "  PHYSHLTH = 99  →  Convert to NaN (Don't know)\n",
    "  WEIGHT2 = 7777 →  Convert to NaN (Refused)\n",
    "  WEIGHT2 = 9999 →  Convert to NaN (Don't know)\n",
    "\n",
    "IMPORTANT CONSIDERATIONS:\n",
    "------------------------\n",
    "  • These codes are NOT missing data in the traditional sense\n",
    "  • They represent specific types of non-response\n",
    "  • Treatment depends on the analysis goals\n",
    "  • Some analyses may want to exclude or include specific codes\n",
    "  • Documentation should clearly state how special values were handled\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF NOTEBOOK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "This notebook provides a complete template for health data mining and analytics\n",
    "projects. It can be adapted for similar public health datasets and extended with\n",
    "additional analyses as needed.\n",
    "\n",
    "For questions or modifications, please refer to the SDS6108 course materials\n",
    "and consult with your course instructor.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}